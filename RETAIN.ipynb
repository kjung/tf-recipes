{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reimplementing RETAIN\n",
    "Notes and test code for setting up model and data for my reimplementation of RETAIN model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os.path\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "from six.moves import xrange\n",
    "\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import from util.py...  \n",
    "from util import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up basic model parameters\n",
    "input_dim = 15851\n",
    "output_dim = 2\n",
    "batch_size = 10\n",
    "max_length = 20\n",
    "embedding_dim = 100\n",
    "hidden_dim = 50\n",
    "data_dir = '/data1/stride6/data'\n",
    "log_dir = '/home/kjung/projects/tf-recipes/logs'\n",
    "EPS = 1e-10\n",
    "l2_coeff = 0.0001\n",
    "init_lr = 0.001\n",
    "\n",
    "# Input placeholders\n",
    "tf.reset_default_graph()\n",
    "input_placeholder = tf.placeholder(tf.float32, shape=[batch_size, max_length, input_dim])\n",
    "target_placeholder = tf.placeholder(tf.float32, shape=[batch_size,])\n",
    "seqlen_placeholder = tf.placeholder(tf.int32, shape=[batch_size,])\n",
    "loss_mask_placeholder = tf.placeholder(tf.float32, shape=[batch_size, max_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Embedding layer\n",
    "embeddings = tf.get_variable('embeddings', \n",
    "                             [input_dim, embedding_dim], \n",
    "                             initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "reshaped_input = tf.reshape(input_placeholder, [-1, input_dim])\n",
    "input_embeddings = tf.matmul(reshaped_input, embeddings)\n",
    "input_embeddings = tf.reshape(input_embeddings, [batch_size, max_length, embedding_dim])\n",
    "\n",
    "# Check on shape\n",
    "print(embeddings)\n",
    "print(input_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define rnn_alpha - an rnn that outputs states and produces scalar weights for each\n",
    "# input embedding.  \n",
    "with tf.variable_scope('rnn_alpha') :\n",
    "    rnn_cell_alpha = tf.nn.rnn_cell.GRUCell(hidden_dim)\n",
    "    alpha_initial_state = rnn_cell_alpha.zero_state(batch_size, tf.float32)\n",
    "    rnn_alpha_output, rnn_alpha_final_state = tf.nn.dynamic_rnn(cell=rnn_cell_alpha, \n",
    "                                                                dtype=tf.float32, \n",
    "                                                                sequence_length=seqlen_placeholder, \n",
    "                                                                initial_state=alpha_initial_state,\n",
    "                                                                inputs=input_embeddings)\n",
    "print(rnn_alpha_output)\n",
    "print(tf.reshape(rnn_alpha_output, [-1, hidden_dim]))\n",
    "      \n",
    "W_alpha = tf.get_variable('W_alpha', \n",
    "                          [hidden_dim,1],\n",
    "                          initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "bias_alpha = tf.get_variable('bias_alpha', [1,])\n",
    "print(W_alpha)\n",
    "print(bias_alpha)\n",
    "\n",
    "# We have variable length sequences, so we can't use the provided softmax function. \n",
    "# Option 1: Do the softmax ourselves, taking care to avoid numerical issues.  \n",
    "# Option 2: Zero out invalid scores, add back eps, then use built-in softmax.  \n",
    "# I suspect option 2 will be faster, so let's try that...\n",
    "alpha_scores = tf.matmul(tf.reshape(rnn_alpha_output, [-1, hidden_dim]), W_alpha) + bias_alpha\n",
    "alpha_scores = tf.reshape(alpha_scores, [batch_size, max_length])\n",
    "alpha_scores = alpha_scores * loss_mask_placeholder + EPS\n",
    "print(alpha_scores)\n",
    "\n",
    "alpha = tf.nn.softmax(alpha_scores)\n",
    "print(alpha)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define rnn_beta\n",
    "with tf.variable_scope('rnn_beta') : \n",
    "    rnn_cell_beta = tf.nn.rnn_cell.GRUCell(hidden_dim)\n",
    "    beta_initial_state = rnn_cell_beta.zero_state(batch_size, tf.float32)\n",
    "    rnn_beta_output, rnn_beta_final_state = tf.nn.dynamic_rnn(cell=rnn_cell_beta, \n",
    "                                                              dtype=tf.float32, \n",
    "                                                              sequence_length=seqlen_placeholder, \n",
    "                                                              initial_state=beta_initial_state,\n",
    "                                                              inputs=input_embeddings)\n",
    "print(rnn_beta_output) \n",
    "W_beta = tf.get_variable('W_beta', \n",
    "                        [hidden_dim, embedding_dim], \n",
    "                        initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "print(W_beta)\n",
    "bias_beta = tf.get_variable('bias_beta', [embedding_dim])\n",
    "\n",
    "beta = tf.tanh( tf.matmul(tf.reshape(rnn_beta_output, [-1, hidden_dim]), W_beta) + bias_beta)\n",
    "beta = tf.reshape(beta, [batch_size * max_length, embedding_dim])\n",
    "print(beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define context node\n",
    "context = beta * tf.reshape(input_embeddings, [-1, embedding_dim])\n",
    "print(context)\n",
    "context = tf.nn.dropout(context, tf.constant(0.5))\n",
    "\n",
    "context = tf.reshape(context, [embedding_dim, batch_size*max_length])\n",
    "print(context)\n",
    "context = tf.transpose(context * tf.reshape(alpha, [-1]))\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prediction node\n",
    "# The tricky thing here is figuring out the last context vector...  Want to go \n",
    "# from [batch_size*max_length, embedding_dim] to [batch_size, embedding_dim], \n",
    "# and from there to a scalar... \n",
    "\n",
    "# TODO - reduce context to right shape : [batch_size, embedding_dim]...  use tf.gather_nd?  \n",
    "\n",
    "W_classify = tf.get_variable('W_classify', \n",
    "                             [embedding_dim,1], \n",
    "                             initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "bias_classify = tf.get_variable('bias_classify', [1])\n",
    "logits = tf.matmul(context, W_classify) + bias_classify\n",
    "print(logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reshape logits to [batch_size, max_length]\n",
    "logits = tf.reshape(logits, [batch_size, max_length])\n",
    "print(logits)\n",
    "logits = tf.gather_nd(logits, tf.pack([tf.range(batch_size), seqlen_placeholder - 1], axis=1))\n",
    "print(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now apply sigmoid to logits to get probs...  \n",
    "output_probs = tf.nn.sigmoid(logits)\n",
    "print(output_probs)\n",
    "\n",
    "# Loss - redo this for 0/1 instead of -1/1 labels\n",
    "classification_loss = -1. * target_placeholder * tf.log(output_probs) + (1. - target_placeholder) * (1. - output_probs)\n",
    "print(classification_loss)\n",
    "classification_loss = tf.reduce_mean(classification_loss)\n",
    "l2_loss = l2_coeff * tf.nn.l2_loss(W_classify)\n",
    "total_loss = classification_loss + l2_loss\n",
    "print(classification_loss)\n",
    "print(total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up to run training... \n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(0.0001)\n",
    "train_op = optimizer.minimize(total_loss, global_step=global_step)\n",
    "print(train_op)\n",
    "\n",
    "sess = tf.Session()\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "foo = np.array([], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bar = np.append(foo, np.array(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]\n"
     ]
    }
   ],
   "source": [
    "print(bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-8]",
   "language": "python",
   "name": "conda-env-tensorflow-8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
